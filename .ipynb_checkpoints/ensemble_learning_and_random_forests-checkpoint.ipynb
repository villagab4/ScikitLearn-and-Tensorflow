{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomFor...bf',\n",
       "  max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Types of ensemble learning\n",
    "hard voting classifier: aggregate predictions of each classifier and predict class that gets most votes\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "# By changing the svm classifier to use probabilities (cross-validation), the voting classifier can use soft voting,\n",
    "# which weights the individual probabilities by their confidence, resulting in more accurate predictions\n",
    "svm_clf = SVC(random_state=42, probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "        voting='soft'\n",
    "    )\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.872\n",
      "SVC 0.888\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# Consider each classifier's accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Bagging is a form of sampled with replacement, while pasting is a form of sampling without replacement.\n",
    "These techniques are used when using the same training algorithm for every predicto, but with different\n",
    "random subsets of the training set for each one. Then, the final ensemble predictor aggregates the \n",
    "individual predictions to determine a suitable prediction/classification.\n",
    "'''\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Setting bootstrap to True results in bagging. For pasting, set it to False. The n_jobs tells sklearn\n",
    "# how many CPU cores to use. The -1 says to use all available cores\n",
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "        max_samples=100, bootstrap=True, n_jobs=-1, random_state=42\n",
    "    )\n",
    "# Bagging classifier automatically uses soft voting if the base classifier can estimate class probabilities\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89866666666666661"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Note, since each base model is trained on a random sample, on average, each model will be trained on\n",
    "roughly 63% of the data (limit approaches 1 - exp(-1) = .63212). As a result, the remaining 37% of \n",
    "the training data (known as out-of-bag (oob) instances) can be used for validation.\n",
    "'''\n",
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "        bootstrap=True, n_jobs=-1, oob_score=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91200000000000003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32352941,  0.67647059],\n",
       "       [ 0.35625   ,  0.64375   ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.06145251,  0.93854749],\n",
       "       [ 0.35465116,  0.64534884],\n",
       "       [ 0.01142857,  0.98857143],\n",
       "       [ 0.98930481,  0.01069519],\n",
       "       [ 0.97409326,  0.02590674],\n",
       "       [ 0.7370892 ,  0.2629108 ],\n",
       "       [ 0.0049505 ,  0.9950495 ],\n",
       "       [ 0.75      ,  0.25      ],\n",
       "       [ 0.82681564,  0.17318436],\n",
       "       [ 0.98461538,  0.01538462],\n",
       "       [ 0.06315789,  0.93684211],\n",
       "       [ 0.00490196,  0.99509804],\n",
       "       [ 0.99004975,  0.00995025],\n",
       "       [ 0.92513369,  0.07486631],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.03409091,  0.96590909],\n",
       "       [ 0.34502924,  0.65497076],\n",
       "       [ 0.91666667,  0.08333333],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.96319018,  0.03680982],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.65420561,  0.34579439],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.0049505 ,  0.9950495 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.19148936,  0.80851064],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00591716,  0.99408284],\n",
       "       [ 0.39751553,  0.60248447],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.23463687,  0.76536313],\n",
       "       [ 0.32777778,  0.67222222],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02906977,  0.97093023],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01183432,  0.98816568],\n",
       "       [ 0.97916667,  0.02083333],\n",
       "       [ 0.88297872,  0.11702128],\n",
       "       [ 0.94054054,  0.05945946],\n",
       "       [ 0.9558011 ,  0.0441989 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.06122449,  0.93877551],\n",
       "       [ 0.98026316,  0.01973684],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01005025,  0.98994975],\n",
       "       [ 0.98857143,  0.01142857],\n",
       "       [ 0.81868132,  0.18131868],\n",
       "       [ 0.45303867,  0.54696133],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.70860927,  0.29139073],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.84745763,  0.15254237],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.60526316,  0.39473684],\n",
       "       [ 0.12222222,  0.87777778],\n",
       "       [ 0.6284153 ,  0.3715847 ],\n",
       "       [ 0.9017341 ,  0.0982659 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.17204301,  0.82795699],\n",
       "       [ 0.90055249,  0.09944751],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00540541,  0.99459459],\n",
       "       [ 0.05780347,  0.94219653],\n",
       "       [ 0.02312139,  0.97687861],\n",
       "       [ 0.34337349,  0.65662651],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.82954545,  0.17045455],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.21276596,  0.78723404],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.95811518,  0.04188482],\n",
       "       [ 0.81675393,  0.18324607],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.25274725,  0.74725275],\n",
       "       [ 0.56470588,  0.43529412],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.04255319,  0.95744681],\n",
       "       [ 0.50543478,  0.49456522],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01704545,  0.98295455],\n",
       "       [ 0.98913043,  0.01086957],\n",
       "       [ 0.20903955,  0.79096045],\n",
       "       [ 0.44692737,  0.55307263],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01570681,  0.98429319],\n",
       "       [ 0.98901099,  0.01098901],\n",
       "       [ 0.25698324,  0.74301676],\n",
       "       [ 0.87719298,  0.12280702],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.78285714,  0.21714286],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00578035,  0.99421965],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.98895028,  0.01104972],\n",
       "       [ 0.99447514,  0.00552486],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.96039604,  0.03960396],\n",
       "       [ 0.99497487,  0.00502513],\n",
       "       [ 0.01092896,  0.98907104],\n",
       "       [ 0.15662651,  0.84337349],\n",
       "       [ 0.95918367,  0.04081633],\n",
       "       [ 0.32727273,  0.67272727],\n",
       "       [ 0.98924731,  0.01075269],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.7251462 ,  0.2748538 ],\n",
       "       [ 0.35632184,  0.64367816],\n",
       "       [ 0.4       ,  0.6       ],\n",
       "       [ 0.85326087,  0.14673913],\n",
       "       [ 0.95918367,  0.04081633],\n",
       "       [ 0.06878307,  0.93121693],\n",
       "       [ 0.8030303 ,  0.1969697 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.03508772,  0.96491228],\n",
       "       [ 0.98342541,  0.01657459],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.0052356 ,  0.9947644 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01932367,  0.98067633],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.95402299,  0.04597701],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.38674033,  0.61325967],\n",
       "       [ 0.27717391,  0.72282609],\n",
       "       [ 0.01117318,  0.98882682],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.30729167,  0.69270833],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99378882,  0.00621118],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98295455,  0.01704545],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00502513,  0.99497487],\n",
       "       [ 0.61271676,  0.38728324],\n",
       "       [ 0.91747573,  0.08252427],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99516908,  0.00483092],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.06930693,  0.93069307],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.05405405,  0.94594595],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.04294479,  0.95705521],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.93229167,  0.06770833],\n",
       "       [ 0.73295455,  0.26704545],\n",
       "       [ 0.61212121,  0.38787879],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.12244898,  0.87755102],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.95731707,  0.04268293],\n",
       "       [ 0.96855346,  0.03144654],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02272727,  0.97727273],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.39664804,  0.60335196],\n",
       "       [ 0.85082873,  0.14917127],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99408284,  0.00591716],\n",
       "       [ 0.00520833,  0.99479167],\n",
       "       [ 0.00540541,  0.99459459],\n",
       "       [ 0.96111111,  0.03888889],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.30808081,  0.69191919],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00595238,  0.99404762],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.96236559,  0.03763441],\n",
       "       [ 0.80246914,  0.19753086],\n",
       "       [ 0.995     ,  0.005     ],\n",
       "       [ 0.00543478,  0.99456522],\n",
       "       [ 0.04411765,  0.95588235],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02808989,  0.97191011],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.05405405,  0.94594595],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.83888889,  0.16111111],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.8988764 ,  0.1011236 ],\n",
       "       [ 0.99450549,  0.00549451],\n",
       "       [ 0.18857143,  0.81142857],\n",
       "       [ 0.22651934,  0.77348066],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00549451,  0.99450549],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.23684211,  0.76315789],\n",
       "       [ 0.95744681,  0.04255319],\n",
       "       [ 0.00507614,  0.99492386],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.43956044,  0.56043956],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.04712042,  0.95287958],\n",
       "       [ 0.0982659 ,  0.9017341 ],\n",
       "       [ 0.97959184,  0.02040816],\n",
       "       [ 0.02185792,  0.97814208],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.35106383,  0.64893617],\n",
       "       [ 0.13793103,  0.86206897],\n",
       "       [ 0.54891304,  0.45108696],\n",
       "       [ 0.65656566,  0.34343434],\n",
       "       [ 0.00558659,  0.99441341],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.64622642,  0.35377358],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.22164948,  0.77835052],\n",
       "       [ 0.81094527,  0.18905473],\n",
       "       [ 0.03012048,  0.96987952],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.86387435,  0.13612565],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00518135,  0.99481865],\n",
       "       [ 0.09137056,  0.90862944],\n",
       "       [ 0.00598802,  0.99401198],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.94318182,  0.05681818],\n",
       "       [ 0.15697674,  0.84302326],\n",
       "       [ 0.95897436,  0.04102564],\n",
       "       [ 0.01630435,  0.98369565],\n",
       "       [ 0.57923497,  0.42076503],\n",
       "       [ 0.07526882,  0.92473118],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.78723404,  0.21276596],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.93684211,  0.06315789],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.23560209,  0.76439791],\n",
       "       [ 0.99418605,  0.00581395],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.84831461,  0.15168539],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.78723404,  0.21276596],\n",
       "       [ 0.92391304,  0.07608696],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.68852459,  0.31147541],\n",
       "       [ 0.4973262 ,  0.5026738 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.88953488,  0.11046512],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.9010989 ,  0.0989011 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.72636816,  0.27363184],\n",
       "       [ 0.10227273,  0.89772727],\n",
       "       [ 0.47560976,  0.52439024],\n",
       "       [ 0.24210526,  0.75789474],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.84782609,  0.15217391],\n",
       "       [ 0.80927835,  0.19072165],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99465241,  0.00534759],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.03431373,  0.96568627],\n",
       "       [ 0.95959596,  0.04040404],\n",
       "       [ 0.94797688,  0.05202312],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.52542373,  0.47457627],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.0106383 ,  0.9893617 ],\n",
       "       [ 0.99453552,  0.00546448],\n",
       "       [ 0.02659574,  0.97340426],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98275862,  0.01724138],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.07894737,  0.92105263],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00564972,  0.99435028],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.13559322,  0.86440678],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.37125749,  0.62874251],\n",
       "       [ 0.0923913 ,  0.9076087 ],\n",
       "       [ 0.23255814,  0.76744186],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.98181818,  0.01818182],\n",
       "       [ 0.17777778,  0.82222222],\n",
       "       [ 0.9902439 ,  0.0097561 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.96511628,  0.03488372],\n",
       "       [ 0.33714286,  0.66285714],\n",
       "       [ 0.98843931,  0.01156069],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.02139037,  0.97860963],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.03076923,  0.96923077],\n",
       "       [ 0.64516129,  0.35483871]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=42, n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is the ~equivalent version of the RFC as a Bagging Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "        n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.112492250999\n",
      "sepal width (cm) 0.0231192882825\n",
      "petal length (cm) 0.441030464364\n",
      "petal width (cm) 0.423357996355\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In a decision tree, the features near the root of the tree are more important than those that\n",
    "appear near the leaves or not at all. A Random Forest computes the average depth of each feature\n",
    "across multiple trees to determine which features are typically more important. This is found in the\n",
    "feature_importances_ variable.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "# Petal length is the most important at 44% while sepal width is the least at 2%\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Boosting is a very popular form of Ensemble learning. It combines several weak learners into a strong learner.\n",
    "The most popular boosting methods are AdaBoost and Gradient Boosting.\n",
    "'''\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(random_state=42, max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's construct a model that uses Gradient Boosting with a Decision Tree as the base predictor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60839441])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to build a Gradient Boosting Regression Tree is with ScikitLearn's class\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=79, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To determine the optimal number of trees, use early stopping.\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Can stop training early by using the warm_start parameter which makes Scikit-Learn keep existing trees when the fit\n",
    "method is called.\n",
    "'''\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1,120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Stacking is another popular ensemble method. Instead of using arbitrary functions to determine the\n",
    "final prediction (such as hard voting), we train a model to make predictions based off the predictions\n",
    "of the base models.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
